Модель глубокого обучения является графом слоев. В Keras модели представляют собой экземпляры класса Model. У Model есть подклассы, например последовательные модели Sequential (подкласс класса Model) — простой стек слоев, отображающих единственный вход в единственный выход.

**Некоторые виды моделей нейронных сетей:**

- Сети с двумя ветвями (two-branch networks).

- Многоголовые сети (multihead networks).

- Входные блоки (inception blocks).

Выбор правильной архитектуры сети — больше искусство, чем наука; и хотя есть несколько проверенных методов и принципов, на которые можно положиться, только практика может помочь стать опытным архитектором нейронных сетей.

**После того как определена архитектура сети, нужно выбрать еще три параметра:**

- **Функцию потерь (целевую функцию)** — количественную оценку, которая будет минимизироваться в процессе обучения. Представляет собой меру успеха в решении стоящей задачи.

- **Оптимизатор** — определяет, как будет изменяться сеть под воздействием функции потерь. Реализует конкретный вариант стохастического градиентного спуска (Stochastic Gradient Descent, SGD).

- **Метрики** — показатели успеха (такие как точность классификации), за которыми будет вестись наблюдение во время обучения и проверки. Обучение, в отличие от потерь, не оптимизируется по данным показателям напрямую. Поэтому от метрик не требуется, чтобы они были дифференцированными.

После выбора функции потерь, оптимизатора и метрик можно использовать встроенные методы compile() и fit(), чтобы начать обучение модели. При желании можно также реализовать собственные циклы обучения.

### Compile

Метод compile() настраивает процесс обучения. Он принимает аргументы с оптимизатором, функцией потерь и метриками (в виде списка).

**Пример вызова compile():**

```Python
model = keras.Sequential([keras.layers.Dense(1)])
model.compile(
	optimizer="rmsprop",
	loss="mean_squared_error",
	metrics=["accuracy"]
)
```

В общем случае нет необходимости прописывать функции потерь, метрики или оптимизаторы с нуля, поскольку Keras предлагает широкий спектр встроенных опций, среди которых наверняка найдется то, что нужно для конкретной ситуации:

- **Оптимизаторы:** 

	- SGD (с импульсом или без). 
	
	- RMSprop.
	
	- Adam.
	
	- Adagrad и др.

- **Функции потерь:**

	- CategoricalCrossentropy.
	
	- SparseCategoricalCrossentropy.
	
	- BinaryCrossentropy.
	
	- MeanSquaredError.
	
	- KLDivergence.
	
	- CosineSimilarity и др.

- **Метрики:** 
	
	- CategoricalAccuracy.
	
	- SparseCategoricalAccuracy.
	
	- BinaryAccuracy. 
	
	- AUC.
	
	- Precision.
	
	- Recall и др.

### Fit

За вызовом compile() следует вызов fit(). Метод fit() реализует собственно цикл обучения. Вот его основные аргументы:

- Данные для обучения (исходные данные и целевые значения). Обычно передаются в виде массивов NumPy или объекта Dataset из библиотеки TensorFlow.

- Количество эпох обучения - сколько раз должен повториться цикл обучения на переданных данных. 

- Размер пакета для использования в каждой эпохе обучения методом градиентного спуска - количество обучающих образцов, учитываемых при вычислении градиентов в одном шаге обновления весов.

**Пример вызова fit():**

```Python
history = model.fit(inputs, targets, epochs=5, batch_size=128)
```